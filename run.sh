python -m torchbeast.monobeast \
    --posemb "gt" \
    --num_actors 16 \
    --use_lstm \
    --discounting 0.99 \
    --entropy_cost 0.001 \
    --batch_size 32 \
    --unroll_length 100 \
    --learning_rate 2e-4 \
    --epsilon 1e-7 \
    --wandb_mode "online" \
    --gpu_n 0 \
    --savedir "/home/jmcho/project/IMPALA_memory_maze/logs"