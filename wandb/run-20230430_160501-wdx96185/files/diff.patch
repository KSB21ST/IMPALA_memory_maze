diff --git a/requirements.txt b/requirements.txt
index 24b473e..69311e2 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,5 +1,5 @@
 gym[atari]>=0.14.0  # Installs gym and atari. Needs to happen at once.
-atari-py==0.2.5  # Version with ROMs.
+atari-py==0.2.50
 gitpython>=2.1  # For logging metadata.
 ## Wrappers
 opencv-python  # for atari
diff --git a/torchbeast/atari_wrappers.py b/torchbeast/atari_wrappers.py
index 749a78e..af6ea9a 100644
--- a/torchbeast/atari_wrappers.py
+++ b/torchbeast/atari_wrappers.py
@@ -296,20 +296,27 @@ def make_atari(env_id, max_episode_steps=None):
 
     return env
 
+
+# def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):
+#     """Configure environment for DeepMind-style Atari.
+#     """
+#     if episode_life:
+#         env = EpisodicLifeEnv(env)
+#     if 'FIRE' in env.unwrapped.get_action_meanings():
+#         env = FireResetEnv(env)
+#     env = WarpFrame(env)
+#     if scale:
+#         env = ScaledFloatFrame(env)
+#     if clip_rewards:
+#         env = ClipRewardEnv(env)
+#     if frame_stack:
+#         env = FrameStack(env, 4)
+#     return env
+
 def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):
     """Configure environment for DeepMind-style Atari.
     """
-    if episode_life:
-        env = EpisodicLifeEnv(env)
-    if 'FIRE' in env.unwrapped.get_action_meanings():
-        env = FireResetEnv(env)
     env = WarpFrame(env)
-    if scale:
-        env = ScaledFloatFrame(env)
-    if clip_rewards:
-        env = ClipRewardEnv(env)
-    if frame_stack:
-        env = FrameStack(env, 4)
     return env
 
 
@@ -320,7 +327,7 @@ class ImageToPyTorch(gym.ObservationWrapper):
 
     def __init__(self, env):
         super(ImageToPyTorch, self).__init__(env)
-        old_shape = self.observation_space.shape
+        old_shape = self.observation_space['image'].shape
         self.observation_space = gym.spaces.Box(
             low=0,
             high=255,
@@ -329,7 +336,8 @@ class ImageToPyTorch(gym.ObservationWrapper):
         )
 
     def observation(self, observation):
-        return np.transpose(observation, axes=(2, 0, 1))
+        # print(observation)
+        return np.transpose(observation['image'], axes=(2, 0, 1))
 
 
 def wrap_pytorch(env):
diff --git a/torchbeast/monobeast.py b/torchbeast/monobeast.py
index 4250a5a..eb8f100 100644
--- a/torchbeast/monobeast.py
+++ b/torchbeast/monobeast.py
@@ -21,9 +21,15 @@ import time
 import timeit
 import traceback
 import typing
+import gym
+import numpy as np
+import wandb
 
 os.environ["OMP_NUM_THREADS"] = "1"  # Necessary for multithreading.
 
+# if 'MUJOCO_GL' not in os.environ:
+#     os.environ['MUJOCO_GL'] = 'egl'
+
 import torch
 from torch import multiprocessing as mp
 from torch import nn
@@ -39,7 +45,9 @@ from torchbeast.core import vtrace
 # yapf: disable
 parser = argparse.ArgumentParser(description="PyTorch Scalable Agent")
 
-parser.add_argument("--env", type=str, default="PongNoFrameskip-v4",
+# parser.add_argument("--env", type=str, default="PongNoFrameskip-v4",
+#                     help="Gym environment.")
+parser.add_argument("--env", type=str, default="memory_maze:MemoryMaze-9x9-ExtraObs-v0",
                     help="Gym environment.")
 parser.add_argument("--mode", default="train",
                     choices=["train", "test", "test_render"],
@@ -50,11 +58,11 @@ parser.add_argument("--xpid", default=None,
 # Training settings.
 parser.add_argument("--disable_checkpoint", action="store_true",
                     help="Disable saving checkpoint.")
-parser.add_argument("--savedir", default="~/logs/torchbeast",
+parser.add_argument("--savedir", default="~/logs/memory_maze",
                     help="Root dir where experiment data will be saved.")
 parser.add_argument("--num_actors", default=4, type=int, metavar="N",
                     help="Number of actors (default: 4).")
-parser.add_argument("--total_steps", default=100000, type=int, metavar="T",
+parser.add_argument("--total_steps", default=100000000, type=int, metavar="T",
                     help="Total environment steps to train for.")
 parser.add_argument("--batch_size", default=8, type=int, metavar="B",
                     help="Learner batch size.")
@@ -66,6 +74,8 @@ parser.add_argument("--num_learner_threads", "--num_threads", default=2, type=in
                     metavar="N", help="Number learner threads.")
 parser.add_argument("--disable_cuda", action="store_true",
                     help="Disable CUDA.")
+parser.add_argument("--gpu_num", default="0",
+                    help="Register gpu number.")
 parser.add_argument("--use_lstm", action="store_true",
                     help="Use LSTM in agent model.")
 
@@ -139,11 +149,12 @@ def act(
         timings = prof.Timings()  # Keep track of how fast things are.
 
         gym_env = create_env(flags)
-        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder="little")
-        gym_env.seed(seed)
+        # seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder="little")
+        # gym_env.seed(seed)
         env = environment.Environment(gym_env)
         env_output = env.initial()
         agent_state = model.initial_state(batch_size=1)
+        print(agent_state)
         agent_output, unused_state = model(env_output, agent_state)
         while True:
             index = free_queue.get()
@@ -344,8 +355,12 @@ def train(flags):  # pylint: disable=too-many-branches, too-many-statements
         logging.info("Not using CUDA.")
         flags.device = torch.device("cpu")
 
+    print("create env")
     env = create_env(flags)
 
+    print("$"*100)
+    print(env.observation_space.shape, env.action_space.n)
+
     model = Net(env.observation_space.shape, env.action_space.n, flags.use_lstm)
     buffers = create_buffers(flags, env.observation_space.shape, model.num_actions)
 
@@ -360,10 +375,14 @@ def train(flags):  # pylint: disable=too-many-branches, too-many-statements
         initial_agent_state_buffers.append(state)
 
     actor_processes = []
-    ctx = mp.get_context("fork")
+    # ctx = mp.get_context("fork")
+    ctx = mp.get_context("spawn")
     free_queue = ctx.SimpleQueue()
     full_queue = ctx.SimpleQueue()
 
+    # act(flags, 0, free_queue, full_queue, model, buffers, initial_agent_state_buffers,)
+    # exit()
+
     for i in range(flags.num_actors):
         actor = ctx.Process(
             target=act,
@@ -384,12 +403,18 @@ def train(flags):  # pylint: disable=too-many-branches, too-many-statements
         env.observation_space.shape, env.action_space.n, flags.use_lstm
     ).to(device=flags.device)
 
-    optimizer = torch.optim.RMSprop(
+    # optimizer = torch.optim.RMSprop(
+    #     learner_model.parameters(),
+    #     lr=flags.learning_rate,
+    #     momentum=flags.momentum,
+    #     eps=flags.epsilon,
+    #     alpha=flags.alpha,
+    # )
+
+    optimizer = torch.optim.Adam(
         learner_model.parameters(),
         lr=flags.learning_rate,
-        momentum=flags.momentum,
         eps=flags.epsilon,
-        alpha=flags.alpha,
     )
 
     def lr_lambda(epoch):
@@ -431,6 +456,7 @@ def train(flags):  # pylint: disable=too-many-branches, too-many-statements
                 to_log = dict(step=step)
                 to_log.update({k: stats[k] for k in stat_keys})
                 plogger.log(to_log)
+                wandb.log(to_log, step=step)
                 step += T * B
 
         if i == 0:
@@ -440,6 +466,7 @@ def train(flags):  # pylint: disable=too-many-branches, too-many-statements
         free_queue.put(m)
 
     threads = []
+    print("before thread")
     for i in range(flags.num_learner_threads):
         thread = threading.Thread(
             target=batch_and_learn, name="batch-and-learn-%d" % i, args=(i,)
@@ -507,7 +534,7 @@ def train(flags):  # pylint: disable=too-many-branches, too-many-statements
 
 def test(flags, num_episodes: int = 10):
     if flags.xpid is None:
-        checkpointpath = "./latest/model.tar"
+        checkpointpath = "../logs/memory_maze/latest/model.tar"
     else:
         checkpointpath = os.path.expandvars(
             os.path.expanduser("%s/%s/%s" % (flags.savedir, flags.xpid, "model.tar"))
@@ -543,7 +570,7 @@ def test(flags, num_episodes: int = 10):
 
 
 class AtariNet(nn.Module):
-    def __init__(self, observation_shape, num_actions, use_lstm=False):
+    def __init__(self, observation_shape, num_actions, use_lstm=True):
         super(AtariNet, self).__init__()
         self.observation_shape = observation_shape
         self.num_actions = num_actions
@@ -559,14 +586,16 @@ class AtariNet(nn.Module):
         self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
 
         # Fully connected layer.
-        self.fc = nn.Linear(3136, 512)
+        # self.fc = nn.Linear(3136, 512)
+        self.fc = nn.Linear(1024, 512)
 
         # FC output size + one-hot of last action + last reward.
         core_output_size = self.fc.out_features + num_actions + 1
 
         self.use_lstm = use_lstm
         if use_lstm:
-            self.core = nn.LSTM(core_output_size, core_output_size, 2)
+            # self.core = nn.LSTM(core_output_size, core_output_size, 2)
+            self.core = nn.LSTM(core_output_size, 256, 2)
 
         self.policy = nn.Linear(core_output_size, self.num_actions)
         self.baseline = nn.Linear(core_output_size, 1)
@@ -635,15 +664,22 @@ class AtariNet(nn.Module):
 Net = AtariNet
 
 
+# def create_env(flags):
+#     return atari_wrappers.wrap_pytorch(
+#         atari_wrappers.wrap_deepmind(
+#             atari_wrappers.make_atari(flags.env),
+#             clip_rewards=False,
+#             frame_stack=True,
+#             scale=False,
+#         )
+#     )
+
 def create_env(flags):
-    return atari_wrappers.wrap_pytorch(
-        atari_wrappers.wrap_deepmind(
-            atari_wrappers.make_atari(flags.env),
-            clip_rewards=False,
-            frame_stack=True,
-            scale=False,
-        )
-    )
+    return atari_wrappers.wrap_pytorch(gym.make(flags.env))
+
+# def create_env(flags):
+#     env = gym.make(flags.env)
+#     return env
 
 
 def main(flags):
@@ -655,4 +691,28 @@ def main(flags):
 
 if __name__ == "__main__":
     flags = parser.parse_args()
+
+    wandb_mode = "online"
+
+    wandb_api_key = "a739164ae7f378b332af7275da8f824e1374258f"
+
+    os.environ["WANDB_MODE"] = wandb_mode
+    os.environ["WANDB_API_KEY"] = wandb_api_key
+
+    os.environ["CUDA_VISIBLE_DEVICES"]= flags.gpu_num
+
+    wandb.login()
+
+    wandb.init(
+        project="IMPALA_memory_maze",
+        entity="agentlab",
+        config=flags,
+        save_code=True,
+        allow_val_change=True,
+    )
+
     main(flags)
+
+    wandb.finish()
+
+
